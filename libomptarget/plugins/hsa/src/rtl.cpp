//===----RTLs/hsa/src/rtl.cpp - Target RTLs Implementation -------- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// RTL for hsa machine
// github: ashwinma (ashwinma@gmail.com)
//
//===----------------------------------------------------------------------===//

#include <algorithm>
#include <assert.h>
#include <cstdio>
#include <dlfcn.h>
#include <elf.h>
#include <ffi.h>
#include <gelf.h>
#include <list>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <vector>
#include <iostream>
#include <fstream>
#include <map>
#include <set>
#include <mutex>

// Header from Clang for gpu grid info (static)
#include "clang/Basic/GpuGridValues.h"

// Header from ATMI interface
#include "atmi_runtime.h"
#include "atmi_interop_hsa.h"
#include "atmi_kl.h"

#include "omptargetplugin.h"

#include "rtl.h"

// The maximum number of threads in a worker warp.
#define WAVEFRONTSIZE clang::AMDGPUGpuGridValues[clang::GPU::GVIDX::GV_Warp_Size]
// the maximum number of teams.
#define MAX_NUM_TEAMS clang::AMDGPUGpuGridValues[clang::GPU::GVIDX::GV_Max_Teams]

// Static limit from previous version

// Let's don't simulate SIMD yet, WAVEFRONTSIZE X THREAD_ABSOLUTE_LIMIT is the local group size
#define THREAD_ABSOLUTE_LIMIT 1024// 2 /* omptx limit (must match threadAbsoluteLimit) */

#define MAX_NUM_WAVES   (MAX_NUM_TEAMS * THREAD_ABSOLUTE_LIMIT / WAVEFRONTSIZE)
#define MAX_NUM_THREADS MAX_NUM_WAVES * WAVEFRONTSIZE

#ifdef OMPTHREAD_IS_WAVEFRONT
  // assume here one OpenMP thread per HSA wavefront
  #define MAX_NUM_OMP_THREADS MAX_NUM_WAVES
#else
  // assume here one OpenMP thread per HSA thread
  #define MAX_NUM_OMP_THREADS MAX_NUM_THREADS
#endif

#ifndef TARGET_NAME
#define TARGET_NAME AMDHSA
#endif

#ifdef OMPTARGET_DEBUG
static int DebugLevel = 0;

#define GETNAME2(name) #name
#define GETNAME(name) GETNAME2(name)
#define DP(...) \
  do { \
    if (DebugLevel > 0) { \
      DEBUGP("Target " GETNAME(TARGET_NAME) " RTL", __VA_ARGS__); \
    } \
  } while (false)
#else // OMPTARGET_DEBUG
#define DP(...) {}
#endif // OMPTARGET_DEBUG

#ifdef OMPTARGET_DEBUG
#define check(msg, status) \
  if (status != ATMI_STATUS_SUCCESS) { \
    /* fprintf(stderr, "[%s:%d] %s failed.\n", __FILE__, __LINE__, #msg);*/ \
    DP(#msg" failed\n"); \
    /*assert(0);*/ \
  } else { \
    /* fprintf(stderr, "[%s:%d] %s succeeded.\n", __FILE__, __LINE__, #msg); */ \
    DP(#msg" succeeded\n"); \
  }
#else
#define check(msg, status) \
{}
#endif

/// Keep entries table per device
struct FuncOrGblEntryTy {
  __tgt_target_table Table;
  std::vector<__tgt_offload_entry> Entries;
};

enum ExecutionModeType {
  SPMD, // constructors, destructors,
        // combined constructs (`teams distribute parallel for [simd]`)
  GENERIC, // everything else
  NONE
};

// Each target kernel is described by the following data structure type code
// which is generated by the compiler and contains the kernel's computation
// properties.
struct TargetKernelCompProperties {
  int8_t ExecutionMode;
  int32_t NumReductionVars;
  int32_t ReductionVarsSize;

  TargetKernelCompProperties(int8_t _ExecutionMode, int32_t _NumReductionVars,
      int32_t _ReductionVarsSize) : ExecutionMode(_ExecutionMode),
          NumReductionVars(_NumReductionVars),
          ReductionVarsSize(_ReductionVarsSize) {}
};

typedef atmi_kernel_t ATMIfunction;
/// Use a single entity to encode a kernel and a set of flags
struct KernelTy {
  ATMIfunction Func;

  // execution mode of kernel
  // 0 - SPMD mode (without master warp)
  // 1 - Generic mode (with master warp)
  int8_t ExecutionMode;

  // Number of reduction variables
  int32_t NumReductionVars;

  // Total size of reduction variables
  int32_t ReductionVarsSize;

  KernelTy(ATMIfunction _Func, TargetKernelCompProperties _CP) : Func(_Func),
      ExecutionMode(_CP.ExecutionMode), NumReductionVars(_CP.NumReductionVars),
          ReductionVarsSize(_CP.ReductionVarsSize) {
            DP("Construct kernelinfo: ExecMode %d, NumReductionVars %d, ReductionVarsSize, %d\n",
                ExecutionMode, NumReductionVars, ReductionVarsSize);
          }
};

/// List that contains all the kernels.
/// FIXME: we may need this to be per device and per library.
std::list<KernelTy> KernelsList;

typedef std::map<kmp_intptr_t, atmi_task_handle_t> DataTaskMapTy;
typedef std::set<atmi_task_handle_t> WaitingTasksTy;

/// Class containing all the device information
class RTLDeviceInfoTy {
  std::vector<FuncOrGblEntryTy> FuncGblEntries;
  int NumberOfiGPUs;
  int NumberOfdGPUs;
public:
  int NumberOfDevices;

  // Data dependency to task dependency mapping
  DataTaskMapTy DataTaskMap;
  WaitingTasksTy WaitingTasks;
  std::mutex DataTaskMapMtx;

  // GPU devices
  atmi_machine_t *Machine;
  std::vector<atmi_place_t> GPUPlaces;
  std::vector<atmi_mem_place_t> GPUMEMPlaces;
  std::vector<hsa_agent_t> HSAAgents;

  // Device properties
  std::vector<int> GroupsPerDevice;
  std::vector<int> ThreadsPerGroup;
  std::vector<int> WavefrontSize;

  // OpenMP properties
  std::vector<int> NumTeams;
  std::vector<int> NumThreads;

  // OpenMP Environment properties
  int EnvNumTeams;
  int EnvTeamLimit;

  //static int EnvNumThreads;
  static const int HardTeamLimit = 1<<16; // 64k
  static const int HardThreadLimit = 1024;
  static const int DefaultNumTeams = 128;
  static const int DefaultNumThreads = 128;

  // Record entry point associated with device
  void addOffloadEntry(int device_id, __tgt_offload_entry entry ){
    assert( device_id < (int)FuncGblEntries.size() && "Unexpected device id!");
    FuncOrGblEntryTy &E = FuncGblEntries[device_id];

    E.Entries.push_back(entry);
  }

  // Return true if the entry is associated with device
  bool findOffloadEntry(int device_id, void *addr){
    assert( device_id < (int)FuncGblEntries.size() && "Unexpected device id!");
    FuncOrGblEntryTy &E = FuncGblEntries[device_id];

    for(unsigned i=0; i<E.Entries.size(); ++i){
      if(E.Entries[i].addr == addr)
        return true;
    }

    return false;
  }

  // Return the pointer to the target entries table
  __tgt_target_table *getOffloadEntriesTable(int device_id){
    assert( device_id < (int)FuncGblEntries.size() && "Unexpected device id!");
    FuncOrGblEntryTy &E = FuncGblEntries[device_id];

    unsigned size = E.Entries.size();

    // Table is empty
    if(!size)
      return 0;

    __tgt_offload_entry *begin = &E.Entries[0];
    __tgt_offload_entry *end = &E.Entries[size-1];

    // Update table info according to the entries and return the pointer
    E.Table.EntriesBegin = begin;
    E.Table.EntriesEnd = ++end;

    return &E.Table;
  }

  // Clear entries table for a device
  void clearOffloadEntriesTable(int device_id){
    assert( device_id < (int)FuncGblEntries.size() && "Unexpected device id!");
    FuncOrGblEntryTy &E = FuncGblEntries[device_id];
    for(std::vector<__tgt_offload_entry>::iterator it = E.Entries.begin();
        it != E.Entries.end(); it++) {
        KernelTy *kernel_info = (KernelTy *)it->addr;
        if (kernel_info->Func.handle != 0ull)
            atmi_kernel_release(kernel_info->Func);
    }
    E.Entries.clear();
    E.Table.EntriesBegin = E.Table.EntriesEnd = 0;
  }

  std::vector<atmi_task_handle_t> getTaskDependencies(int32_t depNum,
    kmp_depend_info_t *depList, int32_t noAliasDepNum,
    kmp_depend_info_t *noAliasDepList) {

    std::vector<atmi_task_handle_t> deps;
    deps.clear();

    for(int i = 0; i < depNum; i++) {
      if(depList[i].flags.in) {
        DataTaskMapMtx.lock();
        DataTaskMapTy::iterator taskMapIter = DataTaskMap.find(depList[i].base_addr);
        if(taskMapIter != DataTaskMap.end()) {
          // in dep; add to the ATMI dep list
          atmi_task_handle_t task = taskMapIter->second;
          deps.push_back(task);
          // remove this task from the waiting list
          // because its successor will be waiting
          // for it anyway
          WaitingTasks.erase(task);
        }
        DataTaskMapMtx.unlock();
      }
    }

    for(int i = 0; i < noAliasDepNum; i++) {
      if(noAliasDepList[i].flags.in) {
        DataTaskMapMtx.lock();
        DataTaskMapTy::iterator taskMapIter = DataTaskMap.find(noAliasDepList[i].base_addr);
        if(taskMapIter != DataTaskMap.end()) {
          // in dep; add to the ATMI dep list
          atmi_task_handle_t task = taskMapIter->second;
          deps.push_back(task);
          // erase this task from the waiting list
          // because its successor will be waiting
          // for it anyway
          WaitingTasks.erase(task);
        }
        DataTaskMapMtx.unlock();
      }
    }

    return deps;
  }

  void setTaskDependency(atmi_task_handle_t task, int32_t depNum,
    kmp_depend_info_t *depList, int32_t noAliasDepNum,
    kmp_depend_info_t *noAliasDepList) {

    for(int i = 0; i < depNum; i++) {
      if(depList[i].flags.out) {
        DataTaskMapMtx.lock();
        // out dep found; update the last task
        DataTaskMap[depList[i].base_addr] = task;
        // add this task to the waiting list
        // because in case it has no further successors
        // we need to wait for it at the next epoch boundary
        WaitingTasks.insert(task);
        DataTaskMapMtx.unlock();
      }
    }

    for(int i = 0; i < noAliasDepNum; i++) {
      if(noAliasDepList[i].flags.out) {
        DataTaskMapMtx.lock();
        // out dep found; update the last task
        DataTaskMap[noAliasDepList[i].base_addr] = task;
        // add this task to the waiting list
        // because in case it has no further successors
        // we need to wait for it at the next epoch boundary
        WaitingTasks.insert(task);
        DataTaskMapMtx.unlock();
      }
    }
  }

  void waitAllPendingTasks() {
    DataTaskMapMtx.lock();
    if(WaitingTasks.empty()) {
        DataTaskMapMtx.unlock();
        return;
    }
    WaitingTasksTy::iterator it = WaitingTasks.begin();
    for(; it != WaitingTasks.end(); it++) {
      atmi_task_wait(*it);
    }
    WaitingTasks.clear();
    DataTaskMapMtx.unlock();
  }

  RTLDeviceInfoTy() : DataTaskMapMtx(),
                      DataTaskMap(),
                      WaitingTasks()
  {
#ifdef OMPTARGET_DEBUG
    if (char *envStr = getenv("LIBOMPTARGET_DEBUG")) {
      DebugLevel = std::stoi(envStr);
    }
    if (!DebugLevel) {
      if (char *envStr = getenv("OFFLOAD_DEBUG")) {
        DebugLevel = std::stoi(envStr);
      }
    }
#endif // OMPTARGET_DEBUG

    DP("Start initializing HSA-ATMI\n");

    atmi_status_t err = atmi_init(ATMI_DEVTYPE_ALL);
    if (err != ATMI_STATUS_SUCCESS) {
      DP("Error when initializing HSA-ATMI\n");
      return;
    }
    atmi_machine_t *machine = atmi_machine_get_info();
    NumberOfiGPUs = machine->device_count_by_type[ATMI_DEVTYPE_iGPU];
    NumberOfdGPUs = machine->device_count_by_type[ATMI_DEVTYPE_dGPU];
    NumberOfDevices = machine->device_count_by_type[ATMI_DEVTYPE_GPU];

    if (NumberOfDevices == 0) {
      DP("There are no devices supporting HSA.\n");
      return;
    }
    else {
      DP("There are %d devices supporting HSA.\n", NumberOfDevices);
    }

    Machine = machine;

    // Init the device info
    FuncGblEntries.resize(NumberOfDevices);
    GPUPlaces.resize(NumberOfDevices);
    GPUMEMPlaces.resize(NumberOfDevices);
    HSAAgents.resize(NumberOfDevices);
    ThreadsPerGroup.resize(NumberOfDevices);
    GroupsPerDevice.resize(NumberOfDevices);
    WavefrontSize.resize(NumberOfDevices);
    NumTeams.resize(NumberOfDevices);
    NumThreads.resize(NumberOfDevices);

    for (int i =0; i<NumberOfDevices; i++) {
      ThreadsPerGroup[i]=RTLDeviceInfoTy::DefaultNumThreads;
      GroupsPerDevice[i]=RTLDeviceInfoTy::DefaultNumTeams;

      DP("Device %d: Initial groupsPerDevice %d & threadsPerGroup %d\n",
          i, GroupsPerDevice[i], ThreadsPerGroup[i]);

      //ATMI API to get gpu and gpu memory place
      GPUPlaces[i] = (atmi_place_t)ATMI_PLACE_GPU(0, i);
      GPUMEMPlaces[i] = (atmi_mem_place_t)ATMI_MEM_PLACE_GPU_MEM(0, i, 0);

      //ATMI API to get HSA agent
      err = atmi_interop_hsa_get_agent(GPUPlaces[i], &(HSAAgents[i]));
      check("Get HSA agents", err);
    }

    // Get environment variables regarding teams
    char *envStr = getenv("OMP_TEAM_LIMIT");
    if (envStr) {
      // OMP_TEAM_LIMIT has been set
      EnvTeamLimit = std::stoi(envStr);
      DP("Parsed OMP_TEAM_LIMIT=%d\n", EnvTeamLimit);
    } else {
      EnvTeamLimit = -1;
    }
    envStr = getenv("OMP_NUM_TEAMS");
    if (envStr) {
      // OMP_NUM_TEAMS has been set
      EnvNumTeams = std::stoi(envStr);
      DP("Parsed OMP_NUM_TEAMS=%d\n", EnvNumTeams);
    } else {
      EnvNumTeams = -1;
    }
  }

  ~RTLDeviceInfoTy(){
    DP("Finalizing the HSA-ATMI DeviceInfo.\n");
    atmi_finalize();

#if 0
    // Free devices allocated entry
    for(unsigned i=0; i<FuncGblEntries.size(); ++i ) {
      for(unsigned j=0; j<FuncGblEntries[i].Entries.size(); ++j ) {
        if(FuncGblEntries[i].Entries[j].addr) {
          DP("Existing addr i:%d j:%d  addr:%p\n",i,j,(void*) FuncGblEntries[i].Entries[j].addr);
          // It is not just the free that is broken, this addr cast also sometimes fails
          KernelTy *KernelInfo = (KernelTy *)FuncGblEntries[i].Entries[j].addr;
          if (KernelInfo->Func) {
            DP("Free address %016llx.\n",(long long unsigned)(Elf64_Addr)KernelInfo->Func);
            free(KernelInfo->Func);
          }
        }
      }
    }
#endif
  }
};

static RTLDeviceInfoTy DeviceInfo;

#ifdef __cplusplus
extern "C" {
#endif

int32_t __tgt_rtl_is_valid_binary(__tgt_device_image *image) {

  // Is the library version incompatible with the header file?
  if (elf_version(EV_CURRENT) == EV_NONE) {
    DP("Incompatible ELF library!\n");
    return 0;
  }

  char *img_begin = (char *)image->ImageStart;
  char *img_end = (char *)image->ImageEnd;
  size_t img_size = img_end - img_begin;

  // Obtain elf handler
  Elf *e = elf_memory(img_begin, img_size);
  if (!e) {
    DP("Unable to get ELF handle: %s!\n", elf_errmsg(-1));
    return 0;
  }

  // Check if ELF is the right kind.
  if (elf_kind(e) != ELF_K_ELF) {
    DP("Unexpected ELF type!\n");
    return 0;
  }
  Elf64_Ehdr *eh64 = elf64_getehdr(e);
  Elf32_Ehdr *eh32 = elf32_getehdr(e);

  if (!eh64 && !eh32) {
    DP("Unable to get machine ID from ELF file!\n");
    elf_end(e);
    return 0;
  }

  uint16_t MachineID;
  if (eh64 && !eh32)
    MachineID = eh64->e_machine;
  else if (eh32 && !eh64)
    MachineID = eh32->e_machine;
  else {
    DP("Ambiguous ELF header!\n");
    elf_end(e);
    return 0;
  }

  elf_end(e);

  switch(MachineID) {
    // old brig file in HSA 1.0P
    case 0:
    // brig file in HSAIL path
    case 44890:
    case 44891:
      break;
    // amdgcn
    case 224:
      break;
    default:
      DP("Unsupported machine ID found: %d\n", MachineID);
      return 0;
  }

  return 1;
}

int __tgt_rtl_number_of_devices(){
  return DeviceInfo.NumberOfDevices;
}

int32_t __tgt_rtl_init_device(int device_id){
  hsa_status_t err;

  // this is per device id init
  DP("Initialize the device id: %d\n", device_id);

  hsa_agent_t  &agent = DeviceInfo.HSAAgents[device_id];

  // get the global one from device side as we use static structure
  int GroupsLimit = MAX_NUM_TEAMS;
  int ThreadsLimit = MAX_NUM_OMP_THREADS/MAX_NUM_TEAMS;

  //DP("Initialize the device, GroupsPerDevice limit: %d\n", MAX_NUM_TEAMS);
  //DP("Initialize the device, ThreadsPerGroup limit: %d\n", MAX_NUM_OMP_THREADS);

  // Get group limit
  //
  uint16_t workgroup_max_dim[3];
  err = hsa_agent_get_info(agent, HSA_AGENT_INFO_WORKGROUP_MAX_DIM, &workgroup_max_dim);
  if (err == HSA_STATUS_SUCCESS) {
    DP("Queried group limit: %d\n", (int)workgroup_max_dim[0]);
    DeviceInfo.GroupsPerDevice[device_id] = workgroup_max_dim[0];

    if ((DeviceInfo.GroupsPerDevice[device_id] > GroupsLimit) ||
        (DeviceInfo.GroupsPerDevice[device_id] == 0)) {
      DP("Capped group limit: %d\n", GroupsLimit  );
      DeviceInfo.GroupsPerDevice[device_id]=GroupsLimit;
    }
  }
  else {
    DP("Default group limit: %d\n", GroupsLimit  );
    DeviceInfo.GroupsPerDevice[device_id] = GroupsLimit;
  }

  // Get thread limit
  //
  hsa_dim3_t grid_max_dim;
  err = hsa_agent_get_info(agent, HSA_AGENT_INFO_GRID_MAX_DIM, &grid_max_dim);
  if (err == HSA_STATUS_SUCCESS) {
    DP("Queried thread limit: %u\n",
        reinterpret_cast<uint32_t*>(&grid_max_dim)[0]/DeviceInfo.GroupsPerDevice[device_id]);

    DeviceInfo.ThreadsPerGroup[device_id] =
      reinterpret_cast<uint32_t*>(&grid_max_dim)[0]/DeviceInfo.GroupsPerDevice[device_id];

    if ((DeviceInfo.ThreadsPerGroup[device_id] > ThreadsLimit) ||
        DeviceInfo.ThreadsPerGroup[device_id] == 0) {
      DP("Capped thread limit: %d\n", ThreadsLimit  );
      DeviceInfo.ThreadsPerGroup[device_id]=ThreadsLimit;
    }
  }
  else {
    DP("Default thread limit: %d\n", ThreadsLimit  );
    DeviceInfo.ThreadsPerGroup[device_id] = ThreadsLimit;
  }

  // Get wavefront size
  uint32_t wavefront_size = 0;
  err = hsa_agent_get_info(agent, HSA_AGENT_INFO_WAVEFRONT_SIZE, &wavefront_size);
  if (err == HSA_STATUS_SUCCESS) {
    DP("Queried wavefront size: %d\n", wavefront_size);
    DeviceInfo.WavefrontSize[device_id] = wavefront_size;
  }
  else {
    DP("Default wavefront size: %d\n", WAVEFRONTSIZE );
    DeviceInfo.WavefrontSize[device_id] = WAVEFRONTSIZE;
  }

  // Adjust teams to the env variables
  if (DeviceInfo.EnvTeamLimit > 0 &&
      DeviceInfo.GroupsPerDevice[device_id] > DeviceInfo.EnvTeamLimit) {
    DeviceInfo.GroupsPerDevice[device_id] = DeviceInfo.EnvTeamLimit;
    DP("Capping max groups per device to OMP_TEAM_LIMIT=%d\n",
        DeviceInfo.EnvTeamLimit);
  }

  // Set default number of teams
  if (DeviceInfo.EnvNumTeams > 0) {
    DeviceInfo.NumTeams[device_id] = DeviceInfo.EnvNumTeams;
    DP("Default number of teams set according to environment %d\n",
        DeviceInfo.EnvNumTeams);
  } else {
    DeviceInfo.NumTeams[device_id] = RTLDeviceInfoTy::DefaultNumTeams;
    DP("Default number of teams set according to library's default %d\n",
        RTLDeviceInfoTy::DefaultNumTeams);
  }

  if (DeviceInfo.NumTeams[device_id] > DeviceInfo.GroupsPerDevice[device_id]) {
    DeviceInfo.NumTeams[device_id] = DeviceInfo.GroupsPerDevice[device_id];
    DP("Default number of teams exceeds device limit, capping at %d\n",
        DeviceInfo.GroupsPerDevice[device_id]);
  }

  // Set default number of threads
  DeviceInfo.NumThreads[device_id] = RTLDeviceInfoTy::DefaultNumThreads;
  DP("Default number of threads set according to library's default %d\n",
          RTLDeviceInfoTy::DefaultNumThreads);
  if (DeviceInfo.NumThreads[device_id] >
      DeviceInfo.ThreadsPerGroup[device_id]) {
    DeviceInfo.NumTeams[device_id] = DeviceInfo.ThreadsPerGroup[device_id];
    DP("Default number of threads exceeds device limit, capping at %d\n",
        DeviceInfo.ThreadsPerGroup[device_id]);
  }

  DP("Device %d: default limit for groupsPerDevice %d & threadsPerGroup %d\n",
      device_id,
      DeviceInfo.GroupsPerDevice[device_id],
      DeviceInfo.ThreadsPerGroup[device_id]);

  DP("Device %d: wavefront size %d, total threads %d x %d = %d\n",
      device_id,
      DeviceInfo.WavefrontSize[device_id],
      DeviceInfo.ThreadsPerGroup[device_id],
      DeviceInfo.GroupsPerDevice[device_id],
      DeviceInfo.GroupsPerDevice[device_id]*DeviceInfo.ThreadsPerGroup[device_id]);

  return OFFLOAD_SUCCESS ;
}

__tgt_target_table *__tgt_rtl_load_binary(int32_t device_id, __tgt_device_image *image){
  size_t img_size = (char*) image->ImageEnd - (char*) image->ImageStart;

  DeviceInfo.clearOffloadEntriesTable(device_id);
  // TODO: is BRIG even required to be supported? Can we assume AMDGCN only?
  int useBrig = 0;

  // We do not need to set the ELF version because the caller of this function
  // had to do that to decide the right runtime to use

  // Obtain elf handler and do an extra check
  {
    Elf *elfP = elf_memory ((char*)image->ImageStart, img_size);
    if(!elfP){
      DP("Unable to get ELF handle: %s!\n", elf_errmsg(-1));
      return 0;
    }

    if( elf_kind(elfP) !=  ELF_K_ELF){
      DP("Invalid Elf kind!\n");
      elf_end(elfP);
      return 0;
    }

    uint16_t MachineID;
    {
      Elf64_Ehdr *eh64 = elf64_getehdr(elfP);
      Elf32_Ehdr *eh32 = elf32_getehdr(elfP);
      if (eh64 && !eh32)
        MachineID = eh64->e_machine;
      else if (eh32 && !eh64)
        MachineID = eh32->e_machine;
      else{
        printf("Ambiguous ELF header!\n");
        return 0;
      }
    }

    switch(MachineID) {
      // old brig file in HSA 1.0P
      case 0:
      // brig file in HSAIL path
      case 44890:
      case 44891:
        {
          useBrig = 1;
        };
        break;
      case 224:
        // do nothing, amdgcn
        break;
      default:
        DP("Unsupported machine ID found: %d\n", MachineID);
        elf_end(elfP);
        return 0;
    }

    DP("Machine ID found: %d\n", MachineID);
    // Close elf
    elf_end(elfP);
  }

   atmi_platform_type_t platform = ( useBrig ? BRIG : AMDGCN );
   void *new_img = malloc(img_size);
   memcpy(new_img, image->ImageStart, img_size);
   atmi_status_t err = atmi_module_register_from_memory((void **)&new_img, &img_size, &platform, 1);

   free(new_img);
   check("Module registering", err);
   new_img = NULL;

   DP("ATMI module successfully loaded!\n");

  // TODO: Check with Guansong to understand the below comment more thoroughly.
  // Here, we take advantage of the data that is appended after img_end to get
  // the symbols' name we need to load. This data consist of the host entries
  // begin and end as well as the target name (see the offloading linker script
  // creation in clang compiler).

   // Find the symbols in the module by name. The name can be obtain by
   // concatenating the host entry name with the target name

   __tgt_offload_entry *HostBegin = image->EntriesBegin;
   __tgt_offload_entry *HostEnd   = image->EntriesEnd;

   for( __tgt_offload_entry *e = HostBegin; e != HostEnd; ++e) {

     if( !e->addr ){
       // FIXME: Probably we should fail when something like this happen, the
       // host should have always something in the address to uniquely identify
       // the target region.
       DP("Analyzing host entry '<null>' (size = %lld)...\n",
           (unsigned long long)e->size);

       __tgt_offload_entry entry = *e;
       DeviceInfo.addOffloadEntry(device_id, entry);
       continue;
     }

     if( e->size ){
       __tgt_offload_entry entry = *e;

       void *varptr;
       uint32_t varsize;
       atmi_mem_place_t place = DeviceInfo.GPUMEMPlaces[device_id];
       err = atmi_interop_hsa_get_symbol_info(place, e->name, &varptr, &varsize);

       if (err != ATMI_STATUS_SUCCESS) {
         DP("Loading global '%s' (Failed)\n", e->name);
         return NULL;
       }

       if (varsize != e->size) {
         DP("Loading global '%s' - size mismatch (%u != %lu)\n", e->name,
             varsize, e->size);
         return NULL;
       }

       DP("Entry point " DPxMOD " maps to global %s (" DPxMOD ")\n",
           DPxPTR(e - HostBegin), e->name, DPxPTR(varptr));
       entry.addr = (void *)varptr;

       DeviceInfo.addOffloadEntry(device_id, entry);

       continue;
     }

     DP("to find the kernel name: %s size: %lu\n", e->name, strlen(e->name));

     atmi_mem_place_t place = DeviceInfo.GPUMEMPlaces[device_id];
     atmi_kernel_t kernel;
     uint32_t kernel_segment_size;
     err = atmi_interop_hsa_get_kernel_info(place, e->name,
                HSA_EXECUTABLE_SYMBOL_INFO_KERNEL_KERNARG_SEGMENT_SIZE,
                &kernel_segment_size);
     // just use the non-hidden args
     kernel_segment_size -= sizeof(atmi_implicit_args_t);

     // each arg is a void * in this openmp implementation
     uint32_t arg_num = kernel_segment_size / sizeof(void *);
     std::vector<size_t> arg_sizes(arg_num);
     for(std::vector<size_t>::iterator it = arg_sizes.begin();
                        it != arg_sizes.end(); it++) {
        *it = sizeof(void *);
     }

     atmi_kernel_create(&kernel, arg_num, &arg_sizes[0],
                        1,
                        ATMI_DEVTYPE_GPU, e->name);

     // Load the kernel's computation properties
     // Default values (in case symbol is missing from cubin file):
     // GENERIC, 0, 0
     struct TargetKernelCompProperties CP(ExecutionModeType::GENERIC, 0, 0);

     //int8_t ExecModeVal = ExecutionModeType::SPMD;
     std::string CPNameStr (e->name);
     CPNameStr += "_property";
     const char *CPName = CPNameStr.c_str();

     void *CPPtr;
     uint32_t varsize;
     err = atmi_interop_hsa_get_symbol_info(place, CPName,
                                                        &CPPtr, &varsize);
     if (err == ATMI_STATUS_SUCCESS) {
       if ((size_t)varsize != sizeof(TargetKernelCompProperties)) {
         DP("Loading global computation properties '%s' - size mismatch (%u != %lu)\n",
            CPName, varsize, sizeof(TargetKernelCompProperties));
         return NULL;
       }

       err = atmi_memcpy(&CP, CPPtr, (size_t) varsize);
       if (err != ATMI_STATUS_SUCCESS) {
         DP("Error when copying data from device to host. Pointers: "
            "host = " DPxMOD ", device = " DPxMOD ", size = %u\n",
            DPxPTR(&CP), DPxPTR(CPPtr), varsize);
         return NULL;
       }
       DP("After loading global for %s ExecMode = %d\n",CPName,CP.ExecutionMode);

       if (CP.ExecutionMode < 0 || CP.ExecutionMode > 1) {
         DP("Error wrong exec_mode value specified in HSA code object file: %d\n",
            CP.ExecutionMode);
         return NULL;
       }
     } else {
       DP("Loading global computation properties '%s' - symbol "
           "missing, using default values: ExecutionMode GENERIC (1), "
           "NumReductionVars 0, ReductionVarsSize 0\n", CPName);
     }

     check("Loading computation property", err);

     KernelsList.push_back(KernelTy(kernel, CP));

     __tgt_offload_entry entry = *e;
     entry.addr = (void *)&KernelsList.back();
     DeviceInfo.addOffloadEntry(device_id, entry);
     DP("Entry point %ld maps to %s\n", e - HostBegin, e->name);
   }

   {// send device environment here
     omptarget_device_environmentTy device_env;

     device_env.num_devices = DeviceInfo.NumberOfDevices;
     device_env.device_num = device_id;

#ifdef OMPTARGET_DEBUG
     if (getenv("DEVICE_DEBUG"))
       device_env.debug_mode = 1;
     else
       device_env.debug_mode = 0;
#endif

     const char * device_env_Name="omptarget_device_environment";
     void *device_env_Ptr;
     uint32_t varsize;

     atmi_mem_place_t place = DeviceInfo.GPUMEMPlaces[device_id];
     err = atmi_interop_hsa_get_symbol_info(place, device_env_Name,
         &device_env_Ptr, &varsize);

     if (err == ATMI_STATUS_SUCCESS) {
       if ((size_t)varsize != sizeof(device_env)) {
         DP("Global device_environment '%s' - size mismatch (%u != %lu)\n",
             device_env_Name, varsize, sizeof(int32_t));
         return NULL;
       }

       err = atmi_memcpy(device_env_Ptr, &device_env, varsize);
       if (err != ATMI_STATUS_SUCCESS) {
         DP("Error when copying data from host to device. Pointers: "
             "host = " DPxMOD ", device = " DPxMOD ", size = %u\n",
             DPxPTR(&device_env), DPxPTR(device_env_Ptr), varsize);
         return NULL;
       }

       DP("Sending global device environment %lu bytes\n", (size_t)varsize);
     } else {
       DP("Finding global device environment '%s' - symbol missing.\n", device_env_Name);
       // no need to return NULL, consider this is a not a device debug build.
       //return NULL;
     }

     check("Sending device environment", err);
   }

   return DeviceInfo.getOffloadEntriesTable(device_id);
}

void *__tgt_rtl_data_alloc(int device_id, int64_t size, void *){
  void *ptr = NULL;
    assert(device_id < (int)DeviceInfo.Machine->device_count_by_type[ATMI_DEVTYPE_GPU] && "Device ID too large");
    atmi_mem_place_t place = DeviceInfo.GPUMEMPlaces[device_id];
    atmi_status_t err = atmi_malloc(&ptr, size, place);
    DP("Tgt alloc data %ld bytes, (tgt:%016llx).\n", size, (long long unsigned)(Elf64_Addr)ptr);
    ptr = (err == ATMI_STATUS_SUCCESS) ? ptr : NULL;
    return ptr;
}

int32_t __tgt_rtl_data_submit(int device_id, void *tgt_ptr, void *hst_ptr, int64_t size,
    int32_t depNum, kmp_depend_info_t *depList, int32_t noAliasDepNum,
    kmp_depend_info_t *noAliasDepList){
    // FIXME: should we sync for all outstanding tasks before this???
    DeviceInfo.waitAllPendingTasks();

    atmi_status_t err;
    assert(device_id < (int)DeviceInfo.Machine->device_count_by_type[ATMI_DEVTYPE_GPU] && "Device ID too large");
    DP("Submit data %ld bytes, (hst:%016llx) -> (tgt:%016llx).\n", size, (long long unsigned)(Elf64_Addr)hst_ptr, (long long unsigned)(Elf64_Addr)tgt_ptr);
    err = atmi_memcpy(tgt_ptr, hst_ptr, (size_t) size);
    if (err != ATMI_STATUS_SUCCESS) {
        DP("Error when copying data from host to device. Pointers: "
                "host = 0x%016lx, device = 0x%016lx, size = %lld\n",
                (Elf64_Addr)hst_ptr, (Elf64_Addr)tgt_ptr, (unsigned long long)size);
        return OFFLOAD_FAIL;
    }
    return OFFLOAD_SUCCESS;
}

int32_t __tgt_rtl_data_retrieve(int device_id, void *hst_ptr, void *tgt_ptr, int64_t size){
    // FIXME: should we sync for all outstanding tasks before this???
    DeviceInfo.waitAllPendingTasks();

    assert(device_id < (int)DeviceInfo.Machine->device_count_by_type[ATMI_DEVTYPE_GPU] && "Device ID too large");
    atmi_status_t err;
    DP("Retrieve data %ld bytes, (tgt:%016llx) -> (hst:%016llx).\n", size, (long long unsigned)(Elf64_Addr)tgt_ptr, (long long unsigned)(Elf64_Addr)hst_ptr);
    err = atmi_memcpy(hst_ptr, tgt_ptr, (size_t) size);
    if (err != ATMI_STATUS_SUCCESS) {
        DP("Error when copying data from device to host. Pointers: "
                "host = 0x%016lx, device = 0x%016lx, size = %lld\n",
                (Elf64_Addr)hst_ptr, (Elf64_Addr)tgt_ptr, (unsigned long long)size);
        return OFFLOAD_FAIL;
    }
    DP("DONE Retrieve data %ld bytes, (tgt:%016llx) -> (hst:%016llx).\n", size, (long long unsigned)(Elf64_Addr)tgt_ptr, (long long unsigned)(Elf64_Addr)hst_ptr);
    return OFFLOAD_SUCCESS;
}

int32_t __tgt_rtl_data_delete(int device_id, void* tgt_ptr) {
    assert(device_id < (int)DeviceInfo.Machine->device_count_by_type[ATMI_DEVTYPE_GPU] && "Device ID too large");
    atmi_status_t err;
    DP("Tgt free data (tgt:%016llx).\n", (long long unsigned)(Elf64_Addr)tgt_ptr);
    err = atmi_free(tgt_ptr);
    if (err != ATMI_STATUS_SUCCESS) {
        DP("Error when freeing CUDA memory\n");
        return OFFLOAD_FAIL;
    }
    return OFFLOAD_SUCCESS;
}

int32_t __tgt_rtl_run_target_team_region(int32_t device_id, void *tgt_entry_ptr,
    void **tgt_args, ptrdiff_t *tgt_offsets, int32_t arg_num, int32_t team_num,
    int32_t thread_limit, uint64_t loop_tripcount,
    int32_t depNum, kmp_depend_info_t *depList, int32_t noAliasDepNum,
    kmp_depend_info_t *noAliasDepList) {

  // atmi_status_t err;
  // not sure why omptarget includes a last NULL arg

  // Set the context we are using
  // update thread limit content in gpu memory if un-initialized or specified from host

  DP("Run target team region thread_limit %d\n", thread_limit);

  // All args are references.
  // Allocate one more pointer for the reduction scratchpad.
  std::vector<void *> args(arg_num + 1);
  std::vector<void *> ptrs(arg_num + 1);

  DP("Arg_num: %d\n", arg_num);
  for (int32_t i = 0; i < arg_num; ++i) {
    ptrs[i] = (void *)((intptr_t)tgt_args[i] + tgt_offsets[i]);
    args[i] = &ptrs[i];
    DP("Offseted base: arg[%d]:" DPxMOD "\n", i, DPxPTR(ptrs[i]));
  }

  KernelTy *KernelInfo = (KernelTy *)tgt_entry_ptr;

  /*
   * Set limit based on ThreadsPerGroup and GroupsPerDevice
   */
  int threadsPerGroup;

  if (thread_limit > 0) {
    threadsPerGroup = thread_limit;
    DP("Setting threads per block to requested %d\n", thread_limit);
    // Add master warp if necessary
    if (KernelInfo->ExecutionMode == GENERIC) {
      threadsPerGroup += DeviceInfo.WavefrontSize[device_id];
      DP("Adding master wavefront: +%d threads\n", DeviceInfo.WavefrontSize[device_id]);
    }
  } else {
    threadsPerGroup = DeviceInfo.NumThreads[device_id];
    DP("Setting threads per block to default %d\n",
        DeviceInfo.NumThreads[device_id]);
    if (0 && KernelInfo->ExecutionMode == GENERIC) {
      // Leave room for the master warp which will be added below.
      // ??? we do not add this back any more, is this abstraction required?
      threadsPerGroup -= DeviceInfo.WavefrontSize[device_id];
      DP("Subtracting master wavefront: -%d threads\n", DeviceInfo.WavefrontSize[device_id]);
    }
  }

  if (threadsPerGroup > DeviceInfo.ThreadsPerGroup[device_id]) {
    threadsPerGroup = DeviceInfo.ThreadsPerGroup[device_id];
    DP("Threads per group capped at device limit %d\n",
        DeviceInfo.ThreadsPerGroup[device_id]);
  }

  DP("Preparing %d threads\n", threadsPerGroup);

/*
  int kernel_limit;
  err = cuFuncGetAttribute(&kernel_limit,
      CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK, KernelInfo->Func);
  if (err == CUDA_SUCCESS) {
    if (kernel_limit < threadsPerGroup) {
      threadsPerGroup = kernel_limit;
      DP("Threads per block capped at kernel limit %d\n", kernel_limit);
    }
  }

  DP("Preparing %d threads\n", threadsPerGroup);
*/

  unsigned num_groups;
  if (team_num <= 0) {
    if (loop_tripcount > 0 && DeviceInfo.EnvNumTeams < 0) {
      if (KernelInfo->ExecutionMode == SPMD) {
        // round up to the nearest integer
        num_groups = ((loop_tripcount - 1) / threadsPerGroup) + 1;
      } else {
        num_groups = loop_tripcount;
      }
      DP("Using %d teams due to loop trip count %" PRIu64 " and number of "
          "threads per block %d\n", num_groups, loop_tripcount,
          threadsPerGroup);
    } else {
      num_groups = DeviceInfo.NumTeams[device_id];
      DP("Using default number of teams %d\n", DeviceInfo.NumTeams[device_id]);
    }
  } else if (team_num > DeviceInfo.GroupsPerDevice[device_id]) {
    num_groups = DeviceInfo.GroupsPerDevice[device_id];
    DP("Capping number of teams to team limit %d\n",
        DeviceInfo.GroupsPerDevice[device_id]);
  } else {
    DP("The team limit is %d\n",
        DeviceInfo.GroupsPerDevice[device_id]);
    num_groups = team_num;
    DP("Using requested number of teams %d\n", team_num);
  }

  void *Scratchpad = NULL;
  size_t ScratchpadSize = KernelInfo->NumReductionVars == 0 ? 0 :
      256 /*space for timestamp*/ +
      threadsPerGroup * KernelInfo->ReductionVarsSize +
      KernelInfo->NumReductionVars * /*padding=*/256;
  if (ScratchpadSize > 0) {
    Scratchpad = __tgt_rtl_data_alloc(device_id, ScratchpadSize, Scratchpad);
#ifdef OMPTARGET_DEBUG
    if (Scratchpad == NULL)
      DP("Failed to allocate reduction scratchpad\n");
#endif
    unsigned timestamp = 0;
    __tgt_rtl_data_submit(device_id, Scratchpad, &timestamp, sizeof(unsigned),
                          0, NULL, 0, NULL);
  }
  args[arg_num] = &Scratchpad;

  // Run on the device.
  atmi_kernel_t kernel = KernelInfo->Func;
  DP("Launch kernel with %d blocks and %d threads\n", num_groups,
     threadsPerGroup);

  // Collect all same-group in-deps together
  // Create a new group for all same-group out-deps?
  std::vector<atmi_task_handle_t> deps = DeviceInfo.getTaskDependencies(depNum,
                                                depList, noAliasDepNum, noAliasDepList);
  ATMI_LPARM_1D(lparm, num_groups*threadsPerGroup);
  lparm->groupDim[0] = threadsPerGroup;
  lparm->synchronous = (depNum+noAliasDepNum > 0) ? ATMI_FALSE : ATMI_TRUE; // FIXME: pass async param instead of this hack
  lparm->groupable = ATMI_FALSE;
  // TODO: CUDA does not look like being synchronous.
  lparm->place = DeviceInfo.GPUPlaces[device_id];
  if(deps.size() > 0) {
    lparm->num_required = deps.size();
    lparm->requires = &deps[0];
  }
  atmi_task_handle_t task = atmi_task_launch(lparm, kernel, &args[0]);

  DeviceInfo.setTaskDependency(task, depNum, depList, noAliasDepNum,
                               noAliasDepList);

  DP("Kernel completed\n");

  if (Scratchpad)
    __tgt_rtl_data_delete(device_id, Scratchpad);

  return OFFLOAD_SUCCESS;
}

int32_t __tgt_rtl_run_target_region(int32_t device_id, void *tgt_entry_ptr,
    void **tgt_args, ptrdiff_t *tgt_offsets, int32_t arg_num,
    int32_t depNum, kmp_depend_info_t *depList, int32_t noAliasDepNum,
    kmp_depend_info_t *noAliasDepList)
{
  // use one team and one thread
  // fix thread num
  int32_t team_num = 1;
  int32_t thread_limit = 0; // use default
  return __tgt_rtl_run_target_team_region(device_id,
      tgt_entry_ptr, tgt_args, tgt_offsets, arg_num, team_num, thread_limit, 0,
      depNum, depList, noAliasDepNum, noAliasDepList);
}

#ifdef __cplusplus
}
#endif

